[
  {
    "objectID": "modules.html",
    "href": "modules.html",
    "title": "Modules",
    "section": "",
    "text": "Here is a list of the course modules."
  },
  {
    "objectID": "modules.html#module-1",
    "href": "modules.html#module-1",
    "title": "Modules",
    "section": "Module 1",
    "text": "Module 1\n\nSequencing Technologies"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to TRGN 515",
    "section": "",
    "text": "This is the central hub for TRGN 515: Next Generation Sequencing.\nPlease select a module from the sidebar to begin."
  },
  {
    "objectID": "modules/1_sequencing_techs/1_sequencing_techs.html",
    "href": "modules/1_sequencing_techs/1_sequencing_techs.html",
    "title": "Assignment 1: Next Generation Sequencing Technologies",
    "section": "",
    "text": "DNA sequencing can be broadly defined as the determination of the identity and order of nucleic acid residues in biological samples. Bioinformatic analysis and sequencing results are greatly affected by the choice of sequencing technlogy. Sequencing technologies can be broadly categorized in three groups:\nWe will use a human genome standard (HG002) that has been extremely well-characterized as our gold standard. The HG002 genome assembly is part of an effort hosted by NIST that includes The Telomere-to-Telomere Consortium, the Human Pangenome Reference Consortium and the Genome in a Bottle Consortium, to sequence, assemble and polish the HG002 (also known as GM24385 and huAA53E0) cell line, creating a human “genome benchmark” for the HG002 reference material that covers all bases of the diploid genome and is perfectly accurate. Hence, their own “Q100” project nickname, which refers to a Phred quality score of 1 error per 10 billion bases.\nSome reference papers regarding this dataset:\nWe will compare the characteristics and SNP calls for three sequencing technologies: Illumina, PacBio HiFi, and Nanopore.",
    "crumbs": [
      "Home",
      "Module 1: Sequencing technologies",
      "Assignment 1: Next Generation Sequencing Technologies"
    ]
  },
  {
    "objectID": "modules/1_sequencing_techs/1_sequencing_techs.html#quality-check-for-short-reads---fastqc",
    "href": "modules/1_sequencing_techs/1_sequencing_techs.html#quality-check-for-short-reads---fastqc",
    "title": "Assignment 1: Next Generation Sequencing Technologies",
    "section": "Quality check for short-reads - FastQC",
    "text": "Quality check for short-reads - FastQC\nThe most widely used tool for visually evaluating fastq data is FastQC. If FastQC is ran without arguments, it will open an interactive GUI version of the software. But in most cases, we will run it in the command line.\nA basic way to call FastQC will the following command:\nfastqc # The main command call\n--extract # Extract files from output\n-o &lt;output_dir&gt; # Directory for output files\n-d &lt;temporary_dir&gt; # Directory for temporary files\n&lt;input.fq&gt; # Input fasta file\nThis code can be embedded within a python script or a cluster slurm batch job. An easy way to run it within bash in a batch script would be:\n#!/bin/bash\n\n#SBATCH --account=msalomon_1385\n#SBATCH --partition=main\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=5G\n#SBATCH --time=1:00:00\n#SBATCH --job-name=fastqc\n#SBATCH --output=/scratch1/USER/temp/fastqc_%j.%a.out\n#SBATCH --error=/scratch1/USER/temp/fastqc_%j.%a.err\n\n# Put the input and output directories into variables for conciseness\nin_dir=/project/msalomon_1385/TRGN_515/1_exp_evolution/data/m_tuberculosis/fastq\nout_dir=/scratch1/USER/1_exp_evolution/fastq_qc\n\n# Then we loop over all files within the directory\n\nfor in_fa in $in_dir/*.fastq.gz; # Initiate loop\n    sample=$(basename $in_fa .fastq.gz) # Set up a sample variable for the output.\n    fastqc -o $out_dir/$sample --extract --dir $out_dir $in_fa\ndone # Finilize loop\nBut remember, this is just one way to run it!",
    "crumbs": [
      "Home",
      "Module 1: Sequencing technologies",
      "Assignment 1: Next Generation Sequencing Technologies"
    ]
  },
  {
    "objectID": "modules/1_sequencing_techs/1_sequencing_techs.html#quality-check-for-long-reads",
    "href": "modules/1_sequencing_techs/1_sequencing_techs.html#quality-check-for-long-reads",
    "title": "Assignment 1: Next Generation Sequencing Technologies",
    "section": "Quality check for long-reads",
    "text": "Quality check for long-reads\nYou can visualize the quality of your long reads using FastQC as you would with your short-reads. However, there’s also specific software built for long-reads. A widely used software is NanoPlot. Another option is LongQC.\n\n\n\n\n\n\nWarning\n\n\n\nIf you use FastQC on very long reads, it may run out of memory. By default, FastQC allocates a very small amount of memory (512MB), which makes it run out of memory with very long reads. The way to fix this issue is to allocate more memory using the --memory flag. So if you use FastQC, allocate 5Gb, which should be more than enough. You can do this with the command:\nfastqc --extract -o &lt;output_dir&gt; --d &lt;temporary_dir&gt; --memory 5000 &lt;input.fq&gt;\n\n\nNanoPlot is equally easy to run. It will give similar output to FastQC, but more centered on read length and base quality, which is often the obsession with Nanopore long reads.\nNanoPlot\n--fastq &lt;input.fq&gt; # Input in fastq file\n-o &lt;output_dir&gt; # Output directory\n--loglength # Set up for log read length in the plots\n\nTask 2: Run FastQC on Illumina reads\n\n\n\n\n\n\nNote\n\n\n\nRun FastqQC on the Illumina reads and answer the questions. Focus only on the following figures:\n\nPer base sequence quality\nPer sequence quality scores\nPer base sequence content\nPer sequence GC content\nPer base N content\nSequence Length Distribution\nOverrepresented sequences\nAdapter Content\n\nAnswer the following questions for each plot:\n\nWhat is the plot representing? (10pts)\nWhat quality issue (if any) is the plot showing? (10pts)\nHow would you solve the issue (if any) represented in the plot? (10pts)\n\nAnswer the questions for the forward reads only.",
    "crumbs": [
      "Home",
      "Module 1: Sequencing technologies",
      "Assignment 1: Next Generation Sequencing Technologies"
    ]
  },
  {
    "objectID": "modules/1_sequencing_techs/1_sequencing_techs.html#qc-for-short-reads---trimmomatic",
    "href": "modules/1_sequencing_techs/1_sequencing_techs.html#qc-for-short-reads---trimmomatic",
    "title": "Assignment 1: Next Generation Sequencing Technologies",
    "section": "QC for short-reads - Trimmomatic",
    "text": "QC for short-reads - Trimmomatic\nMultiple tools are commonly used to perform QC on short-reads: 1. Trimmomatic - https://github.com/usadellab/Trimmomatic 2. fastp - https://github.com/OpenGene/fastp 3. Trim Galore! - https://github.com/FelixKrueger/TrimGalore\nWe will use Trimmomatic for this. To remove adapters, you need to provide a file with the adapters for the Illumina machine the reads come from. If you installed Trimmomatic through a conda distribution, that should be within: anaconda3/share/trimmomatic/adapters/. You can also find it at their github page.\nTo run trimmomatic:\ntrimmomatic PE # Main trimmomatic function for paired-ends reads\n&lt;input_1.fastq.gz&gt; # Forward reads\n&lt;input_2.fastq.gz&gt; # Reverse reads\n&lt;output_1.fastq.gz&gt; # Output forward reads\n&lt;output_unclassified_1.fastq.gz&gt; # Output unclassified reads from forward file\n&lt;output_2.fastq.gz&gt; # Output reverse reads\n&lt;output_unclassified_2.fastq.gz&gt; # Output unclassified reads from reverse file\nSLIDINGWINDOW:&lt;window_size&gt;:&lt;quality&gt; # Sliding window for quality trimming.\nLEADING:&lt;quality&gt; # Remove leading bases under quality\nTRAILING:&lt;quality&gt; # Remove trailing bases under quality\nAVGQUAL:&lt;quality&gt; # Average quality to remove a read\nMINLEN:&lt;length&gt; # Minimum length to remove a read\nILLUMINACLIP:&lt;/path/to/adapter_file.fa&gt;:&lt;seed_mismatches&gt;:palindrome_clip_threshold&gt;:&lt;simple_clip_threshold&gt;\nFor the SLIDINGWINDOW argument, a window of window_size size moves along the read. If the average quality of the window falls below quality, the remaining of the read is clipped. This prevents reads to be removed just because of a few bad bases. This argument assumes the quality will only get worse at the end of the read.\nThe LEADING and TRAILING arguments will start at the begonning and end of the read and keep removing bases as long as their quality is below quality.\nFor ILLUMINACLIP, we need to provide the file for the adapters. This depends on the Illumina machine. The seed_mismatches argument specifies the maximum number of base mismatches allowed within a short initial sequence from the read (“seed”, 16bp) used to identify potential adapter matches, allowing for a small degree of mismatch when searching for adapters to clip. Trimmomatic uses two strategies to find adapters: the sample mode, and the palindromic mode. simple_clip_threshold is the minimum score threshold for the adapter to align to the read for clipping to take place in the simple mode. Suggested values are 7-15. palindrome_clip_threshold specifies how accurate the match between the two ‘adapter ligated’ reads must be for paired-end palindrome read alignment. Suggested values are around 30. For more information, these two strategies are extensively explained in the Trimmomatic publication.\nFor our dataset, we can run Trimmomatic within bash as follows:\nin_dir=/project/msalomon_1385/TRGN_515/1_seq_techs/illumina\nout_dir=/scratch1/USER/1_exp_evolution/fastq_pass\nadapters=/path/to/conda/share/trimmomatic/adapters/TruSeq3-PE.fa\n\n# Unlike FastQC, we need to run both forward and reverse reads together\n\nfor file in $in_dir/*_1.fastq.gz; # Initiate loop with forward reads only\n    sample=$(basename $in_fa _1.fastq.gz) # Set up a sample variable for input and output.\n    fwd=$in_dir/${sample}_1.fastq.gz # Forward read\n    rev=$in_dir/${sample}_2.fastq.gz # Reverse read\n    trimmomatic PE $fwd $rev $out_dir/${sample}_1.fastq.gz /dev/null $out_dir/${sample}_2.fastq.gz /dev/null SLIDINGWINDOW:4:5 LEADING:5 TRAILING:5 AVGQUAL:5 MINLEN:35 ILLUMINACLIP:$adapters:2:30:10\n    # Note we send unclassified reads to /dev/null\ndone # Finilize loop\n\nTask 3: Run FastQC after running Trimmomatic\n\n\n\n\n\n\nNote\n\n\n\nCompare the following FastqQC plots before and after:\n\nPer base sequence quality\nPer sequence quality scores\nPer base sequence content\nPer sequence GC content\nPer base N content\nSequence Length Distribution\nOverrepresented sequences\nAdapter Content\n\nAnswer the following questions for each plot:\n\nWhat has changed after running trimmomatic? (10pts)\nWhy did the change happen? (10pts)",
    "crumbs": [
      "Home",
      "Module 1: Sequencing technologies",
      "Assignment 1: Next Generation Sequencing Technologies"
    ]
  },
  {
    "objectID": "modules/1_sequencing_techs/1_sequencing_techs.html#short-read-mapping",
    "href": "modules/1_sequencing_techs/1_sequencing_techs.html#short-read-mapping",
    "title": "Assignment 1: Next Generation Sequencing Technologies",
    "section": "Short-read mapping",
    "text": "Short-read mapping\n\nBWA\nTo run any read mapping software, we first need a reference genome. We will use the Human Reference Genome version GRCh38. You can find the CARC location for this file at the beginning of this document.\nMost modern mappers require building an index to easily parse the reference genome during mapping. In fact, some of the biggest speed and memory improvements over early mappers was thanks to improved indexing methods. If they don’t require an index file, it’s because they index the reference genome on the fly. BWA, for instance, is based on the Burrows-wheeler Transform for powerful indexing to quickly locate where a read might align within the reference genome. The indexing for BWA can be done with the following command:\nbwa index $ref_genome\nThat will create multiple files with the same name as your reference genome, but ending in different extensions. Those are the indexes. Keep those files with the same and location as your reference genome. If a software requires those indexes, you just need to show the location of the reference genome, and it will assume the name and location of all the indexes.\nOn CARC there’s already a folder with the indexed GRCh38 reference genome, so we don’t need to do this step.\nThe basic use of BWA requires only the indexed reference genome and the files for the reads (or single file if not paired-end). To run BWA, do:\nbwa mem # Main call for the mem algorithm within BWA\n&lt;reference_genome&gt; # Reference genome\n&lt;fastq1&gt; # Forward reads\n&lt;fastq2&gt; # Reverse reads\nAn important part of the BAM file is the Read Group, especially for Illumina reads. Read groups provide technical information about flowcell, lane and multiplexing of illumina reads. This is important for finding library and sequencing issues, as well as for removing duplicates (next section!). Read Groups are added within the header with the tag RG. To find the flowcell and lane information you can use the headers of a fastq file. For modern fastq files:\nflowcell=$(zcat &lt;file.fq.gz&gt; | head -n1 | cut -d':' -f3)\nlane=$(zcat &lt;file.fq.gz&gt; | head -n1 | cut -d':' -f4)\nTwo common flags to add are the Read Group ID abd the Platform unit. You can define them as:\nrg_id=$flowcell.$lane\npu=$flowcell.$lane.$sample\nWhere $sample is your sample name. To add read groups on bwa mem, use the -R flag.\nbwa mem -R \"@RG\\tID:$rg_id\\tPU:$pu\\tPL:$pl\\tSM:$sample\" \nThe read group ID and platform information allows us to find library and sequencing bias. The sample name makes the BAM and VCF file a lot more concise later on, as the sample name is extracted from there.\nFastq files generated with older machines have different headers. You can find more information about the Fastq headers here (https://en.wikipedia.org/wiki/FASTQ_format) and about Read groups in the GATK website (https://gatk.broadinstitute.org/hc/en-us/articles/360035890671-Read-groups).\nNOTE: Some fastq files may have more than one flowcell and lane for all the reads if the sample was sequenced in multiple flowcell lanes. In that case, if you detect bias in the Per Tile plot in FastQC, you may have to separate the reads by flowcell and lane before mapping.\nThe output of bwa is sometimes unsorted. To ensure we have a coordinate sorted bam file (sorted by starting position), do the following:\nsamtools sort # Main call\n-@ &lt;n&gt; # Number of threads\n-T &lt;path&gt; # Temporary directory\n-O bam # Output in bam format\n-o &lt;path&gt; # Output directory\nintput_bam # Input bam file\nsamtools sort allows for other types of sorting, such as read name sorting or tag-based sorting, but coordinate sorting is by far the most common and expected by downstream bioinformatic processes.\nMany tools will require to index the output bam file for efficiency. This can be done with:\nsamtools index bam_file;\nWe can combine everything into one command using bash pipes:\nbwa mem &lt;reference_genome&gt; &lt;fastq1_path&gt; &lt;fastq2_path&gt; | samtools view -bS - | samtools sort -@ &lt;n&gt; -T &lt;path&gt; -O bam -o &lt;out_bam_path&gt;\n\nsamtools index &lt;out_bam_path&gt;",
    "crumbs": [
      "Home",
      "Module 1: Sequencing technologies",
      "Assignment 1: Next Generation Sequencing Technologies"
    ]
  },
  {
    "objectID": "modules/1_sequencing_techs/1_sequencing_techs.html#long-read-mapping",
    "href": "modules/1_sequencing_techs/1_sequencing_techs.html#long-read-mapping",
    "title": "Assignment 1: Next Generation Sequencing Technologies",
    "section": "Long-read mapping",
    "text": "Long-read mapping\nThe most widely long-read mapper is minimap2, written by the same person who developed BWA, Heng Li. As BWA, the command line command to run it is quite simple:\nminimap2\n&lt;reference_genome&gt;\n&lt;long_reads.fastq.gz&gt;\nminimap2 does not require you to provide an index, as it does it very efficienty on the fly.\nA useful feature of minimap2 is that it comes with some presets depending on the sequencing platform and the type of read. These presets take into account the typical read length and sequencing error profile for each technology.\n\nTask 5: Map reads to the reference genome\n\n\n\n\n\n\nNote\n\n\n\nUse bwa mem for short reads and minimap2 for long-reads. For minimap2, choose the appropiate platform in the presets.\nIn both cases, create a pipe with the output of the mapper and samtools for coordinate sorting.\nFor the final output, any read that maps to chromosomes other than the chromosome you are working with should be considered ambiguous. Use the right command to keep only reads that mapped to your chromosome of interest.\n\nCreate a slurm array job to run the previous code. Attach your code to the assignment (10pts)",
    "crumbs": [
      "Home",
      "Module 1: Sequencing technologies",
      "Assignment 1: Next Generation Sequencing Technologies"
    ]
  },
  {
    "objectID": "modules/1_sequencing_techs/1_sequencing_techs.html#variant-calling-in-short-reads---bcftools",
    "href": "modules/1_sequencing_techs/1_sequencing_techs.html#variant-calling-in-short-reads---bcftools",
    "title": "Assignment 1: Next Generation Sequencing Technologies",
    "section": "Variant calling in short-reads - BCFtools",
    "text": "Variant calling in short-reads - BCFtools\nFor Illumina reads, we will use BCFtools for SNP calling.\nBcftools is ran in two steps. First, we create a pileup of the alignment. A pileup is a file where aligned reads are parsed and summarized at each specific genomic position. Thus, each genomic position is scanned and reads that spanned that position are gathered (piled up!), summarizing that position with metrics such as read depth, base quality, position bias, etc. It’s basically like finding variants but without applying any specific model. Many variant calling tools use some sort of pileup, even if they don’t do it explicitly as bcftools. Typically, the output of the pileup will be pippeted to bcftools variant calling function, which applies a model to calculate the likelihood of a variant and a genotype, given the pileup at that position. Variants are often stored in a Variant Calling Format (VCF) file.\nTo run the pileup and bcftools for variant discovery:\nbcftools mpileup # Pileup function\n-f &lt;reference_genome&gt;\n-a AD,INFO/AD,ADF,INFO/ADF,ADR,INFO/ADR,DP,SP # Add tags useful for filtering\n-q &lt;n&gt; # Skip reads with read quality below n\n-Q &lt;n&gt; # Skip bases with base quality below n\n-Ou # Uncompressed output\n&lt;input_bam&gt;\n&lt;output_vcf&gt;\nOnce the pileup is done, the second step involves running the actual variant calling model. The main variant calling function can be used as:\nbcftools call # Main bcftools call function\n-m # Multiallelic calling method\n-Oz # Output in compressed format\n&lt;output.vcf.gz&gt;\nAfter calling variants, the file needs to be normalized. Variant normalization ensures genetic variants are represented in a consistent way across different data sets.\nbcftools norm\n-f &lt;reference_genome&gt;\n-Oz # Output in compressed format\n-o &lt;output.vcf.gz&gt; # Output file name\n&lt;input.vcf.gz&gt; # Input file name\nAfter calling and normalizing variants, we can soft-filter them to tag variants that we deem of low quality. Soft-filtering means that we won’t remove low-quality variants, we will just add a tag to them.\nbcftools filter # Main function\n-m+ # Append filter tags to previous tags\n# Then each tag is added depending on the condition tested, and we piped it to more filtering commands\n-s'MinMQ' # Tag to add\n-e 'INFO/MQ &lt; 20' # Condition to add tag\n&lt;input.vcf.gz&gt; | # Input file name\nbcftools filter -m+ -s'QUAL' -e 'QUAL &lt; 20' |\nbcftools filter -m+ -s'minAD' -e 'FMT/AD[:1] &lt; 10' |\n# ... as many filters as you want |\nbcftools filter -m+ -s'minADF' -e 'FMT/ADF[:1] &lt; 3' |\n-Oz # Output in compressed format\n-o &lt;output.vcf.gz&gt; # Output file name\nMany tools will require the VCF file to be indexed. This can be done as follows:\ntabix -p vcf &lt;vcf_file&gt;\n\nTask 6: Complete the Variant Calling command:**\n\n\n\n\n\n\nNote\n\n\n\nPipe the commands mpileup, call, norm and filter into one command to stream from the input bam file to the final VCF output.\nbcftools mpileup | ...\nUse the commands following this instructions. You will have to look at the help for each command:\n\nFor bcftools mpileup:\n\nUse all reads regardless of mapping quality\nKeep only bases with base quality higher or equal than 20\n\nFor bcftools call:\n\nPrint variant sites only\nDo not report indels\nKeep all alternate alleles\n\n\nApply the following filter tags in bcftools filter:\n\n‘MinMQ’ for MQ lower than 20\n‘QUAL’ for QUAL lower than 20\n‘minAD’ for allele depth lower than 20\n‘minADF’ for allele forward depth lower than 5\n‘minADR’ for allele reverse depth lower than 5\n‘MinDP’ for total depth lower than 50\n\n\n\nYour output should be a VCF with the same number of variants as before. Remember, we have not removed any variant, we have only added a filter tag in the FILTER column. Variants that pass all our filters will automatically be assigned the tag “PASS” in the FILTER column.",
    "crumbs": [
      "Home",
      "Module 1: Sequencing technologies",
      "Assignment 1: Next Generation Sequencing Technologies"
    ]
  },
  {
    "objectID": "modules/1_sequencing_techs/1_sequencing_techs.html#long-read-variant-calling",
    "href": "modules/1_sequencing_techs/1_sequencing_techs.html#long-read-variant-calling",
    "title": "Assignment 1: Next Generation Sequencing Technologies",
    "section": "Long-read variant calling",
    "text": "Long-read variant calling\nTechnically, there’s no need to use different software for long-read variant calling. However, many variant callers have been developed with long-reads in mind, and benchmarking studies show they can perform better than general variant callers. Also, long-read mapping is characterized by alignments full of small artefactual indels, which can make short-read variant callers struggle and take a long time.\n\nTask 7: Call SNPs against the human reference genome**\n\n\n\n\n\n\nNote\n\n\n\nTo be consistent between sequencing platforms, we will use a variant caller that’s platform agnostic (i.e: it’s not optimized for any especific sequencing technology or read type). We will use BCFtools.\nYou can choose any variant caller you prefer, but run the same caller for all three sequencing technologies (Illumina, PacBio HiFi, ONT). Use any filters you see necessary.\nFor this task, report your variant calling command and your choice of options and filters.\n\nCreate a slurm array job to run the SNP calling code. Call only SNPs. Attach your code to the assignment (5pts)",
    "crumbs": [
      "Home",
      "Module 1: Sequencing technologies",
      "Assignment 1: Next Generation Sequencing Technologies"
    ]
  },
  {
    "objectID": "modules/1_sequencing_techs/1_sequencing_techs.html#task-1-compare-raw-read-statistics-between-sequencing-technologies",
    "href": "modules/1_sequencing_techs/1_sequencing_techs.html#task-1-compare-raw-read-statistics-between-sequencing-technologies",
    "title": "Assignment 1: Next Generation Sequencing Technologies",
    "section": "Task 1: Compare raw read statistics between sequencing technologies",
    "text": "Task 1: Compare raw read statistics between sequencing technologies\n\n\n\n\n\n\nNoteTask 1: Compare raw read statistics between sequencing technologies\n\n\n\nFor this first task, let’s extract some basic metrics from the raw fastq files and compare between sequencing technologies. To be efficient, let’s take a random sample of 200,000 reads per sequencing technology.\nThe metrics we will report are:\n\nRead length\nAverage Base quality per read\nGC content per read\n\nOne way to do it using one of the previously shown software is:\nseqtk sample -s 100 # random seed\n&lt;fastq_file&gt; # Input fastq file\n&lt;n_reads&gt; # Number of reads to downselect\n| seqkit fx2tab # Main seqkit function to get read stats\n-q # Get average read quality\n-l # Get read length\n-g # Get GC content\n-n # Only print read name\n- &gt; &lt;output_file.txt&gt;\n\nPlot a histogram of each of the three metrics. Plot all illumina, ont and pacbio together. (10pts)\nDescribe briefly the differences in read length, base quality and GC content per sequencing platform (10pts)",
    "crumbs": [
      "Home",
      "Module 1: Sequencing technologies",
      "Assignment 1: Next Generation Sequencing Technologies"
    ]
  },
  {
    "objectID": "modules/1_sequencing_techs/1_sequencing_techs.html#sambamba",
    "href": "modules/1_sequencing_techs/1_sequencing_techs.html#sambamba",
    "title": "Assignment 1: Next Generation Sequencing Technologies",
    "section": "Sambamba",
    "text": "Sambamba\nSambamba takes a bam file as input and outputs a bam file where the read flag has been changed in some reads to reflect they are a duplicate. You can run sambamba as:\nsambamba markdup # Main call to the duplicate marking function\n--tmpdir=&lt;temp_dir&gt;\n&lt;input.bam&gt;\n&lt;output.bam&gt;\nUnfortunately, sambamba (as picard and most tools for duplicate detection) require multiple passes of the input bam file, and therefore can’t read from stdin and be used in a pipe.\nFor a efficient and fast software that can be included within pipes, you can use samtools markdup. To run samtools markup you need to do a couple of operations to ensure proper read sorting and tagging:\nsamtools collate -Ou &lt;input_bam&gt; | # Order reads by read-pairs\nsamtools fixmate -m - - -u | # Add mate coordinates and score tags\nsamtools sort - -u | # Order reads by coordinate\nsamtools markdup - &lt;output_bam&gt; # Mark duplicates\nThis code can be pipped into your previous mapping command into one nice and efficient stream!",
    "crumbs": [
      "Home",
      "Module 1: Sequencing technologies",
      "Assignment 1: Next Generation Sequencing Technologies"
    ]
  },
  {
    "objectID": "modules/1_sequencing_techs/1_sequencing_techs.html#marking-duplicates",
    "href": "modules/1_sequencing_techs/1_sequencing_techs.html#marking-duplicates",
    "title": "Assignment 1: Next Generation Sequencing Technologies",
    "section": "Marking Duplicates",
    "text": "Marking Duplicates\nThe term “duplicates” usually refers to identical sequences that appear more than once in the data. Duplicated sequences can be genuine, meaning that they are created due to real duplicated fragments in the genome. However, these duplicates can also occur due to library prep and sequencing artifacts. High rate of artificial duplicates can impact variant detection, as a variant may seem supported by many independent reads when in reality it’s just an error artificially duplicated.\nThe two most common sources for artificial duplicates in Illumina data are: * PCR duplicates - They occur when the same fragment of DNA gets amplify multiple times during library preparation, often due to overamplification * Optical duplicates - They occur during sequencing when a single amplification fluorescent cluster is incorrectly detected as multiple clusters by the optical sensor of the sequencing instrument.\nNOTE: Duplicate marking is not always required! Some amplicon sequencing approaches will have sequencing reads start at the same sites (the primer sites), and therefore could be marked as duplicates without representing errors. Also, in RNA-seq you would also expect high rates of sample duplication, and therefore is also not advised to remove duplicates (unless you have UMIs…)\nMost tools for marking duplicates from BAM files use the 5’ coordinates and mapping orientations of each read (or read pair), including any clipping, gaps or insertions, finding all reads that share exact genomic coordinates. It then keeps the read with the highest quality, marking the rest as duplicates. Note that often reads are not removed, but a flag is added. That’s why we call it marking duplicates!\nThe most commonly used software for marking duplicates is Picard (also implemented within GATK). However, this tool requires a lot of memory and time. I prefer another implementation of the same algorithm within the software Sambamba.\n\nSambamba\nSambamba takes a bam file as input and outputs a bam file where the read flag has been changed in some reads to reflect they are a duplicate. You can run sambamba as:\nsambamba markdup # Main call to the duplicate marking function\n--tmpdir=&lt;temp_dir&gt;\n&lt;input.bam&gt;\n&lt;output.bam&gt;\nUnfortunately, sambamba (as picard and most tools for duplicate detection) require multiple passes of the input bam file, and therefore can’t read from stdin and be used in a pipe.\nFor a efficient and fast software that can be included within pipes, you can use samtools markdup. To run samtools markup you need to do a couple of operations to ensure proper read sorting and tagging:\nsamtools collate -Ou &lt;input_bam&gt; | # Order reads by read-pairs\nsamtools fixmate -m - - -u | # Add mate coordinates and score tags\nsamtools sort - -u | # Order reads by coordinate\nsamtools markdup - &lt;output_bam&gt; # Mark duplicates\nThis code can be pipped into your previous mapping command into one nice and efficient stream!",
    "crumbs": [
      "Home",
      "Module 1: Sequencing technologies",
      "Assignment 1: Next Generation Sequencing Technologies"
    ]
  }
]