[
  {
    "objectID": "modules/3_annotation/3_annotation.html",
    "href": "modules/3_annotation/3_annotation.html",
    "title": "Assignment 3: Variant consequence annotation",
    "section": "",
    "text": "Functional annotation of variants involve predicting the potential functional impact of a genetic variant on a gene or protein. Common annotations include the variant consequence (synonymous, non-synonymous), the variant status (pathogenic, non-pathogenic, uncertain, etc), its location (intron, exon, TTS, etc.), aminoacid location and change, etc.\nIn this module, we will use 2 datasets:",
    "crumbs": [
      "Home",
      "Module 3: Variant functional annotation",
      "Assignment 3: Variant consequence annotation"
    ]
  },
  {
    "objectID": "modules/3_annotation/3_annotation.html#input-and-outputs",
    "href": "modules/3_annotation/3_annotation.html#input-and-outputs",
    "title": "Assignment 3: Variant consequence annotation",
    "section": "Input and outputs",
    "text": "Input and outputs\nThe input for this assessment are the fastq files for all sequenced strains.\n\nFastq files\n/project2/msalomon_1816/trgn_515/3_annotation/m_tuberculosis/fastq\n\n\nH37Rv hypervariable regions\n/project2/msalomon_1816/trgn_515/3_annotation/m_tuberculosis/reference/mtb.h37rv.hypervariable.bed\n\nThe final output will be a list of variants likely to be associated with rifampicin resistance.",
    "crumbs": [
      "Home",
      "Module 3: Variant functional annotation",
      "Assignment 3: Variant consequence annotation"
    ]
  },
  {
    "objectID": "modules/3_annotation/3_annotation.html#required-software",
    "href": "modules/3_annotation/3_annotation.html#required-software",
    "title": "Assignment 3: Variant consequence annotation",
    "section": "Required software",
    "text": "Required software\nIn order to complete the assessment, the following tools need to be installed:\nmodule load bwa\nmodule load htslib\nmodule load bcftools\nmodule load samtools\nmodule load fastqc",
    "crumbs": [
      "Home",
      "Module 3: Variant functional annotation",
      "Assignment 3: Variant consequence annotation"
    ]
  },
  {
    "objectID": "modules/3_annotation/3_annotation.html#slurm-arrays",
    "href": "modules/3_annotation/3_annotation.html#slurm-arrays",
    "title": "Assignment 3: Variant consequence annotation",
    "section": "SLURM arrays",
    "text": "SLURM arrays\nFor this assignment we are going to work with multiple samples, applying the same commands to each of them, so it’s worth learning how to properly use SLURM arrays. Job arrays allow you to submit a single SLURM job script to launch many similar jobs. Since we will be running the same bioinformatic pipeline, array jobs are perfect for this task.\nTo submit an array job, you only need to add this line to the SLURM job #SBATCH --array=N0-N1, but changing N0 and N1 for your starting and ending number, respectively (eg: #SBATCH --array=1-10 to run 10 jobs). The only thing that changes between each of the jobs in the array is the variable SLURM_ARRAY_TASK_ID, which takes the value of each iteration that you specified with #SBATCH --array=1-10. So in the first iteration, the variable ${SLURM_ARRAY_TASK_ID} will have a value of 1, then of 2, 3 etc. So that variable is the only element that lets you change what happens in each iteration, so you need to use it in a smart way! We can use it to select specific files in a folder, or a specific line in a file. For instance:\n#!/bin/bash\n\n#SBATCH --account=msalomon_1385\n#SBATCH --partition=main\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=5G\n#SBATCH --time=1:00:00\n#SBATCH --job-name=fastqc\n#SBATCH --output=/scratch1/USER/temp/fastqc_%j.%a.out\n#SBATCH --error=/scratch1/USER/temp/fastqc_%j.%a.err\n#SBATCH --array=1-10\n\n# Put the input and output directories into variables for conciseness\nin_dir=/project2/msalomon_1816/trgn_515/3_annotation/m_tuberculosis/fastq\nout_dir=/scratch1/USER/1_exp_evolution/fastq_qc\n\n# First, let's select one file within a directory. In each iteration we will get a different file:\nin_file=$(ls $in_dir/ | awk \"NR==${SLURM_ARRAY_TASK_ID}\")\n\nsample=$(basename $in_file .fastq.gz) # Set up a sample variable for the output.\nfastqc -o $out_dir/$sample --extract --dir $out_dir $in_file\nIn this example, we are listing the files in our input directory using ls, then we are selecting each file depending on their order using awk and ${SLURM_ARRAY_TASK_ID}. So when ${SLURM_ARRAY_TASK_ID} is 1, we get the first file; when it’s 2, we get the second file, etc.\n# Now let's use a file with sample ids in each line of the file\nsample_list=/scratch1/USER/1_exp_evolution/sample_list.txt\nsample=$(awk \"NR==${SLURM_ARRAY_TASK_ID}\" $sample_list)\n\nin_file=$sample.fastq.gz # Set up the file name variable.\nfastqc -o $out_dir/$sample --extract --dir $out_dir $in_file\nIn this case, we directly have a list of sample ids that we use the array job to loop through.\nThere are many ways to build array jobs, and the way to structure it will depend on your specific application. Just remember, everything comes down to how you use the ${SLURM_ARRAY_TASK_ID} variable.\nA couple of other interesting variables in the array header are %j and %a. The variable %j stores the job id, and the variable %a has the array index (1,2,3,…n), so each of the stderr and stdout outputs will have their own name.",
    "crumbs": [
      "Home",
      "Module 3: Variant functional annotation",
      "Assignment 3: Variant consequence annotation"
    ]
  },
  {
    "objectID": "modules/3_annotation/3_annotation.html#short-read-alignment",
    "href": "modules/3_annotation/3_annotation.html#short-read-alignment",
    "title": "Assignment 3: Variant consequence annotation",
    "section": "Short-read alignment",
    "text": "Short-read alignment\nTo run any read mapping software, we first need a reference genome. In the previous assignments, the CARD server already had multiple human reference genomes for us to use. For M. tuberculosis, we need to get it ourselves. The most common reference genome for tuberculosis is H37Rv. The assembly name for this reference genome is ASM19595v2.\nAll modern mappers require building an index to easily parse the reference genome during mapping. In fact, some of the biggest speed and memory improvements over early mappers was thanks to improved indexing methods. BWA, for instance, is based on the Burrows-wheeler Transform for powerful indexing to quickly locate where a read might align within the reference genome. The indexing for BWA can be done with the following command:\nbwa index $ref_genome\nThat will create multiple files with the same name as your reference genome, but ending in different extensions. Those are the indexes. Keep those files with the same and location as your reference genome. If a software requires those indexes, you just need to show the location of the reference genome, and it will assume the name and location of all the indexes.\n\nTask 1: Complete the mapping and variant calling pipeline for M. tuberculosis.\n\n\n\n\n\n\nNote\n\n\n\n\nFind the reference genome assembly ASM19595v2 in NCBI, download it, and fill in variable ref_genome in your mapping script ref_genome=/path/to/reference\nIndex the reference genome\nPerform QC in the raw reads\nPrepare a SLURM array job to perform the entire mapping pipeline:\n\nQuality control and filtering\nMapping\nMarking duplicates\nIndel re-alignment\nVariant calling\nVariant soft-filtering\nIndex the vcf file (tabix -p vcf &lt;input_file.vcf.gz&gt;)\n\n\nAttach your SLURM array job script as the answer to this question (20pts)",
    "crumbs": [
      "Home",
      "Module 3: Variant functional annotation",
      "Assignment 3: Variant consequence annotation"
    ]
  },
  {
    "objectID": "modules/3_annotation/3_annotation.html#multi-sample-vcf-files",
    "href": "modules/3_annotation/3_annotation.html#multi-sample-vcf-files",
    "title": "Assignment 3: Variant consequence annotation",
    "section": "Multi-sample VCF files",
    "text": "Multi-sample VCF files\nWhen comparing samples, we often use multi-sample VCF files. The file structure is the same as the single-sample files we have used in the previous modules, but now we will have multiple SAMPLE columns. This is useful for quickly comparing variants between samples, but it also save disk space and file number when dealing with many samples.\n\nTask 2: Merge the VCF files from the T85 strain and the CDC strain**.\n\n\n\n\n\n\nNote\n\n\n\nFind the right bcftools tool to merge the 4 VCF files from each lineage, resulting in two VCF files with 4 samples each.\nNOTE: Remember to use the right flag within the command to remove non-pass variants!\n\nPaste your variant merging command (5pts)\nDescribe each column of the resulting VCF file. How does it differ from a single-sample VCF file? (5pts)\nShould we use the INFO column or the FORMAT column to filter specific samples? Why? (5pts)",
    "crumbs": [
      "Home",
      "Module 3: Variant functional annotation",
      "Assignment 3: Variant consequence annotation"
    ]
  },
  {
    "objectID": "modules/3_annotation/3_annotation.html#filtering-low-complexity-regions",
    "href": "modules/3_annotation/3_annotation.html#filtering-low-complexity-regions",
    "title": "Assignment 3: Variant consequence annotation",
    "section": "Filtering low complexity regions",
    "text": "Filtering low complexity regions\nLow complexity regions are very hard to map correctly, especially when using short-reads. For that reason, often we remove any variants that fall within known regions.\nIf you don’t have a file with hypervariable regions, you can infer them! The most classic algorithm to do this is DustMasker from NCBI (https://www.ncbi.nlm.nih.gov/IEB/ToolBox/CPP_DOC/lxr/source/src/app/dustmasker/). There are faster implementations of the same algorithm. I can recommend Heng Li’s implementation sdust, which is available in the minimap2 installation.\nFor this assignment, I have provided you with a file of hypervariable regions. Look at the path of file in the “Input and outputs” section.\nThere are many ways to remove variants that fall within specific genomic coordinates.\n\nHard filtering\n\nWe can use bcftools for this. We can use the flag -T. Using it on its own will keep regions listed in the file. Using it with the symbol ^ will remove regions listed in the file.\nSometimes, to use bcftools and quickly jump between lines, we need to first compress and index both the VCF and the BED files. This is useful when using flags like -r or -R:\n# Compress and index the VCF\nbgzip input.vcf\ntabix -p vcf input.vcf.gz\n\n# Compress and index the BED file\nbgzip regions.bed\ntabix -p bed regions.bed.gz\nFor the -T flag, bcftools actually reads the file line by line, so we don’t need to index it.\nWe can then use the -T flag with the ^ symbol in front of the region file:\nbcftools view -T ^regions.bed input.vcf.gz -O z -o hard_filtered.vcf.gz\nAnother quick option if you want to avoid compressing and indexing file is using bedtools:\nbedtools intersect -header -v -a input.vcf -b regions.bed &gt; hard_filtered.vcf\n\nSoft filtering\n\nThis is a bit more complex than hard filtering, since we are adding a conditional tag to the FILTER column.\nFirst, we need to devine what our new tag is in the VCF header:\necho '##FILTER=&lt;ID=hypervar,Description=\"Variant falls within a hypervariable region\"&gt;' &gt; filter_header.txt\nWe can then use bcftools annotate to add that filter in the VCF file. We will use the --mark-sites (-m) flag to append the tag to the existing FILTER options. We will also use the -h flag to append the new FILTER header line to the existing header.\nbcftools annotate -a regions.bed -h filter_header.txt -m '+hypervar' input.vcf.gz -O z -o soft_filtered.vcf.gz\n\nTask 3: Removing variants that fall within hypervariable regions.\n\n\n\n\n\n\nNote\n\n\n\n\nHard-filtered the variants from your multi-sample VCF that fall within the hypervariable regions in the .bed file. Attach your command (5pts)",
    "crumbs": [
      "Home",
      "Module 3: Variant functional annotation",
      "Assignment 3: Variant consequence annotation"
    ]
  },
  {
    "objectID": "modules/3_annotation/3_annotation.html#variant-functional-annotation",
    "href": "modules/3_annotation/3_annotation.html#variant-functional-annotation",
    "title": "Assignment 3: Variant consequence annotation",
    "section": "Variant functional annotation",
    "text": "Variant functional annotation\nFunctional annotation of variants involve predicting the potential functional impact of a genetic variant on a gene or protein. Common annotations include the variant consequence (synonymous, non-synonymous), it’s location (intron, exon, TTS, etc.), aminoacid location and change, etc.\nVariant functional consequence prediction typically requires an annotation file. This annotation file comes in a variety of formats and with varying information. The most used formats are GFF (General Feature Format) and GTF (Gene Transfer Format), where gene location and coordinates are the main source of information. These formats are pretty similar to the VCF format, and many fields will seem familiar.\n\nTask 4: Variant annotation file\n\n\n\n\n\n\nNote\n\n\n\n\nFind in NCBI and download the annotation file in GFF format for ASM19595v2, and fill in variable gff_file (5pts) gff_file=/path/to/gff_file\n\n\n\nThese annotation files sometimes come unsorted. To ensure proper formatting of the GFF file and to index it, we can use genometools with the following code:\nconda activate /project2/msalomon_1816/trgn_515/conda_env/genometools\n\ngt gff3 -sort -tidy -retainids input.gff | bgzip -c &gt; sorted.gff.gz\ntabix index -p gff sorted.gff.gz\nUsing genometools ensures proper formatting. GFF format stores genomic features (gene, mRNA, exon…). These features need to be ordered hierarchically, with parent features (e.g gene) preceding their child features (e.g mRNA, exon, CDS). Using genometools ensures that the GFF file is sorted by coordinate while respecting the hierarchical order.\nWe can also sort it using bedtools, but this won’t strictly enforce the hierarchical order of genomic features:\nbedtools sort -header -i input.gff | bgzip -c &gt; sorted.gff.gz\nWe can also use simple bash scripting, but this command will remove the header, which may cause problems with some tools:\ngrep -v \"#\" $gff_file | sort -k1,1 -k4,4n -k5,5n -t$'\\t' | bgzip -c &gt; $gff_file.gz\ntabix -p gff $gff_file.gz\nThere are many tools to perform variant annotation. Some of the most commonly used include VEP (Variant Effect Predictor, from Ensembl); SnpEff; BCFtools csq, or ANNOVAR. Ideally, we would like a simple software that takes a VCF file as an input, gives an annotated VCF file as output, and can be easily used with command pipes.\nTo use VEP, use the conda environment:\nconda activate /project2/msalomon_1816/trgn_515/conda_env/vep\nFor bcftools or SnpEff:\nconda activate /project2/msalomon_1816/trgn_515/conda_env/trgn515\n\nVEP\nVEP is a very popular choice as it’s one of the most comprehensive tools and it has a seamless integration with the Ensembl databases. It is, howver, one of the slowest softwares for variant annotation, so you should be careful when using it for large genomes (such as the human one).\nvep # Main call to VEP\n-i &lt;vcf_file|stdin&gt;\n-o &lt;vcf_file|stdout&gt;\n--gff &lt;gff_file&gt;\n--fasta &lt;reference_genome&gt;\n-distance &lt;n&gt; # Include consequences in genes n basepairs upstream and downstream. Useful for variants in regulatory regions or terciary structures \n--vcf # Output in vcf format\n--compress_output bgzip # Compress output\n--no_stats # Do not report stats file\n\n\nBCFtools csq\nAs most bcftools tools, BCFtools csq is memory efficient and incredibly fast. Additionally, unlike most other tools, BCFtools csq predicts consequences within a genotype. That means that if a variant corrects another variant dowstream, the consequence of both variants will be reverted.\nbcftools csq\n-f &lt;ref_genome&gt;\n-g &lt;gff_file&gt;\n-p a # Handle unphased genotypes\n-Oz\n-o &lt;vcf_file|stdout&gt;\n&lt;vcf_file|stdin&gt;\nUnfortunately, BCFtools csq is very particular about the format of the GFF file, and it requires that the file adheres perfectly to the Ensembl format. That means that even files downloaded from NCBI may cause errors. For most Eukaryotic genomes, you can find GFF files on the Ensembl portal.\n\n\nSnpEff\nSnpEff is another widely used variant prediction tool. The performance is slightly better than VEP, but a lot worse than BCFtools csq. However, in some cases it offers more detailed variant prediction.\nsnpEff eff\n&lt;database_name&gt;\n&lt;input_vcf_file|stdin&gt; \nThe database name for M. tuberculosis H37Rv is Mycobacterium_tuberculosis_h37rv. If your genome is not present in their standard databases, you can build your own, but this process is quite cumbersome.\n\n\n\nTask 5: Variant functional annotation\n\n\n\n\n\n\nNote\n\n\n\n\nUse one of VEP, bcftools, or SnpEff to annotate the variants from the multi-sample filtered VCF file (5pts)\n\n\n\n\nVariant consequence\nThere can be many different types of variant consequences in our output. Some good resources to check what those consequences mean:\n\nhttps://useast.ensembl.org/info/genome/variation/prediction/predicted_data.html\nhttp://sequenceontology.org/browser/current_svn/term/\n\nFor easy visualizaiton of the results, it is a good idea to parse the VCF file into other formats.\nThe final data we need to finish the assignment is gene name, variant consequence and what samples that variant is present.\nYou can write a script to parse the VCF file and gather that information. For easy interpretation of results, let’s assume any position with heterozygous variants is subjected to sequencing errors. A quick way to do this within bcftools would be bcftools query.\nFor instance (considering output from VEP):\npaste &lt;(bcftools query -f '%CHROM\\t%POS\\t%REF\\t%ALT[\\t%SAMPLE=%GT]\\n' CDC_all.vcf.gz) &lt;(bcftools query -f '%CSQ\\n' CDC_all.vcf.gz | cut -d '|' -f4,2 | tr '|' '\\t') | egrep -v \"0/1\" &gt; CDC_results.txt\n\npaste &lt;(bcftools query -f '%CHROM\\t%POS\\t%REF\\t%ALT[\\t%SAMPLE=%GT]\\n' T85_all.vcf.gz) &lt;(bcftools query -f '%CSQ\\n' T85_all.vcf.gz | cut -d '|' -f4,2 | tr '|' '\\t') | egrep -v \"0/1\" &gt; T85E_results.txt\nWe can also use the bcftools plugin bcftools +split-vep. There are many plugins to bcftools. These are usually for specific tasks outside of the main bcftools functionality.\nTo use these plugins, you need to call them using the + sign in front of the plugin name. You can get a list of all available plugins using:\nbcftools plugin -l\nWe can use this plugin to select the features we want from the VCF file. For the VEP output:\nbcftools view -e 'GT=\"0/1\"' CDC_all.vcf.gz | \\\nbcftools +split-vep -d -f '%CHROM\\t%POS\\t%REF\\t%ALT[\\t%SAMPLE=%GT]\\t%Consequence\\t%SYMBOL\\n' &gt; CDC_results.txt\nYou will have to adapt this to bcftools csq or snpeff.\nYou can use the split-vep plugin to get a list of the subfields so you can modify the previous command to adapt it to bcftools or snpeff:\n# For SnpEff\nbcftools +split-vep -a ANN -l CDC_all.vcf.gz\n\n# For bcftools csq\nbcftools +split-vep -a BCSQ -l CDC_all.vcf.gz\n\n\n\nTask 6: Parse required information from VCF files\n\n\n\n\n\n\nNote\n\n\n\nUse any of the options listed above to parse the variant functional annotation from the VCF file.\n\nAttach your parsing command to the assignment (5pts)\n\n\n\n\n\nTask 7: Detect variants likely to be associated to rifampicin resistance.\n\n\n\n\n\n\nNote\n\n\n\nAnswer the following questions:\n\nWhat criteria does a variant need to meet to be a good candidate for rifampicin resistance in our experiment? (10pts)\nWhat are the genes with the highest number of variants? (5pts)\nWhat are the genes with the highest number of non-synoymous variants? In how many samples are these mutations present? (5pts)\nWhat are the genes with the highest proportion of non-synoymous variants to synonymous variants? (5pts)\nReview the variants within those genes in IGV and determine if the alignment is reliable (5pts)\nWhat antibiotic resistant mechanism from the ones reviewed in class explains the functional consequence of the variant? (i.e: target site modification, efflux pump, etc.) (10pts)",
    "crumbs": [
      "Home",
      "Module 3: Variant functional annotation",
      "Assignment 3: Variant consequence annotation"
    ]
  },
  {
    "objectID": "modules/3_annotation/3_annotation.html#input-and-outputs-1",
    "href": "modules/3_annotation/3_annotation.html#input-and-outputs-1",
    "title": "Assignment 3: Variant consequence annotation",
    "section": "Input and outputs",
    "text": "Input and outputs\nFor this dataset, we will start using VCF files with high confidence variant calls.\n\nVCF files\n/project2/msalomon_1816/trgn_515/3_annotation/HCC1395_breast_cancer/vcf/high-confidence_sSNV_in_HC_regions_v1.2.1.vcf.gz",
    "crumbs": [
      "Home",
      "Module 3: Variant functional annotation",
      "Assignment 3: Variant consequence annotation"
    ]
  },
  {
    "objectID": "modules/3_annotation/3_annotation.html#using-clinical-databases",
    "href": "modules/3_annotation/3_annotation.html#using-clinical-databases",
    "title": "Assignment 3: Variant consequence annotation",
    "section": "Using clinical databases",
    "text": "Using clinical databases\n\nClinVar\nwget https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/clinvar.vcf.gz\ntabix -p vcf clinvar.vcf.gz\nbcftools annotate \\\n  -a clinvar.vcf.gz \\\n  -c INFO/CLNSIG \\\n  -Oz -o tumor_annotated.vcf.gz \\\n  tumor_gnomad.vcf.gz\n\n\ngnomAD\nIn clinical genomics, variants that are common in the general population are almost always filtered out when looking for rare pathogenic drivers. We can pull the Allele Frequency (AF) from gnomAD and annotate our VCF file with it.\nbcftools annotate \\\n  -a gnomad.genomes.vcf.gz \\\n  -c ID,INFO/AF \\\n  -Oz -o &lt;output_vcf.gz&gt; \\\n  &lt;input.vcf.gz&gt;",
    "crumbs": [
      "Home",
      "Module 3: Variant functional annotation",
      "Assignment 3: Variant consequence annotation"
    ]
  },
  {
    "objectID": "modules/3_annotation/3_annotation.html#in-silico-score-prediction",
    "href": "modules/3_annotation/3_annotation.html#in-silico-score-prediction",
    "title": "Assignment 3: Variant consequence annotation",
    "section": "In silico score prediction",
    "text": "In silico score prediction",
    "crumbs": [
      "Home",
      "Module 3: Variant functional annotation",
      "Assignment 3: Variant consequence annotation"
    ]
  },
  {
    "objectID": "modules/2_sv/2_sv.html",
    "href": "modules/2_sv/2_sv.html",
    "title": "Assignment 2: Structural variants",
    "section": "",
    "text": "The first assignment focused on single-nucleotide variants (SNV or SNP), where a single nucleotide is substituted for another. For this assignment, we will detect other types of genetic variation knowns as Indels and Structural Variants:\nTraditionally, indels are considered to be from 1bp to 50bp. This distinction is a bit arbitrary and is mostly based on early alignment tools, which mostly extended gaps (deletions) of up to around 50bp and where thus available to variant callers that relied on alignment statistics reported in the CIGAR string within the BAM file. Biologically, indels are typically caused by Replication slippage, while structural variants are caused by more complex mechanisms, including Nonallelic homologous recombination or Non-homologous end-joining (NHEJ). In reality, there’s no real consensus of when a variant is considered an indel and when it is a large insertion or deletion. Usually, indels are detected by variant callers that also detect SNPs (using the information about the alignment) and structural variants are detected by specialized software.\nWe will again use the human genome standard (HG002) as our gold standard, and we will compare our indels and SV against the set of variants present when comparing HG002 against the reference genome. For indels, we will use the same benchmarking set of variants against the GRCh38 human assembly. For SV, however, there’s only the set of variants in reference to the GRCh37, so we have to use that assembly instead.\nSome reference papers regarding this dataset:\nWe will compare the characteristics and SNP calls for three sequencing technologies: Illumina, PacBio HiFi, and Nanopore.",
    "crumbs": [
      "Home",
      "Module 2: Structural Variants",
      "Assignment 2: Structural variants"
    ]
  },
  {
    "objectID": "modules/2_sv/2_sv.html#illumina",
    "href": "modules/2_sv/2_sv.html#illumina",
    "title": "Assignment 2: Structural variants",
    "section": "Illumina",
    "text": "Illumina\nUse one of the following variant callers: BCFtools, GATK, or FreeBayes.\n# For bcftools or freebayes\nconda activate /project2/msalomon_1816/trgn_515/conda_env/trgn515\n\n# For GATK\nconda activate /project2/msalomon_1816/trgn_515/conda_env/gatk\nFilters (some of these nomenclature may vary by caller):\n\nMQ &lt; 20\nQUAL &lt; 20\nDP &lt; 20\nAD: if homozoygous, 10 reads; if heterozygous, 5 reads\neg: ‘FMT/GT ~ “1/1” & FMT/AD[:1] &lt; 10’\nYou will have to complete the heterozygous filter on your own",
    "crumbs": [
      "Home",
      "Module 2: Structural Variants",
      "Assignment 2: Structural variants"
    ]
  },
  {
    "objectID": "modules/2_sv/2_sv.html#ont-and-pacbio",
    "href": "modules/2_sv/2_sv.html#ont-and-pacbio",
    "title": "Assignment 2: Structural variants",
    "section": "ONT and PacBio",
    "text": "ONT and PacBio\nUse Nanocaller.\nconda activate /project2/msalomon_1816/trgn_515/conda_env/nanocaller\nNanoCaller ...\nFilters (some of these nomenclature mary vary by caller):\n\nMQ &lt; 10\nDP &lt; 20\nAD: if homozoygous, 10 reads; if heterozygous, 5 reads\n\nNOTE: Use the Human GRCh38 assembly as reference genome.\n\nCreate a slurm job to run the Indel calling code. Attach your code to the assignment (5pts)",
    "crumbs": [
      "Home",
      "Module 2: Structural Variants",
      "Assignment 2: Structural variants"
    ]
  },
  {
    "objectID": "modules/0_intro_git/0_intro_git.html",
    "href": "modules/0_intro_git/0_intro_git.html",
    "title": "Module 0: Git and GitHub",
    "section": "",
    "text": "By the end of this module, you will be able to:\n\nConfigure your local Git environment.\nClone your class repository.\nUse the core Git commands (add, commit, push) to submit an assignment.",
    "crumbs": [
      "Home",
      "Module 0: Sequencing technologies",
      "Module 0: Git and GitHub"
    ]
  },
  {
    "objectID": "modules/0_intro_git/0_intro_git.html#learning-objectives",
    "href": "modules/0_intro_git/0_intro_git.html#learning-objectives",
    "title": "Module 0: Git and GitHub",
    "section": "",
    "text": "By the end of this module, you will be able to:\n\nConfigure your local Git environment.\nClone your class repository.\nUse the core Git commands (add, commit, push) to submit an assignment.",
    "crumbs": [
      "Home",
      "Module 0: Sequencing technologies",
      "Module 0: Git and GitHub"
    ]
  },
  {
    "objectID": "modules/0_intro_git/0_intro_git.html#step-1-join-the-trgn-515-classroom",
    "href": "modules/0_intro_git/0_intro_git.html#step-1-join-the-trgn-515-classroom",
    "title": "Module 0: Git and GitHub",
    "section": "Step 1: Join the TRGN-515 classroom",
    "text": "Step 1: Join the TRGN-515 classroom\nBefore starting, you need to accept the assignment via GitHub Classroom using the following link:\nhttps://classroom.github.com/a/nMEEuL7F\nIf you already have a github account, you can link it.\nIf you don’t have a github account, you need to create one. Clicking the link will direct you to log in or create an account. Once you create your account, verify your account using the code sent to your email.\nOnce you accept the link, authorize GitHub classroom.\nFinally, you have to select your username from the roster. Accept the assignment. Your repository will be automatically created.\nYou now have a repository on our github classroom. You can access it with the following link:\nhttps://github.com/trgn-515/USERNAME.\nTo change the name of this repository: 1) click on your repository; 2) Settings &gt; Repository name; 3) Change and click “Rename” twice.\n\n\n\n\n\n\nImportant\n\n\n\nFor security reasons, many of the github operations we will do require you to create a token. A token is like a temporary password.\nTo create a token, go to your github account, click user (upper-right icon) &gt; Settings &gt; Developer Settings (all the way down on the left column) &gt; Personal access tokens &gt; Tokens (Classic) &gt; Generate new token &gt; Generate new token (classic)\nWrite a name for the token, change the expiration to “No expiration” for convenience and select “repo” scopes. Click Generate token at the bottom of the page. For all GitHub operations, this token is your password. Copy this token. You can create as many tokens as you want.",
    "crumbs": [
      "Home",
      "Module 0: Sequencing technologies",
      "Module 0: Git and GitHub"
    ]
  },
  {
    "objectID": "modules/0_intro_git/0_intro_git.html#step-2-connect-git-repository-to-the-cluster",
    "href": "modules/0_intro_git/0_intro_git.html#step-2-connect-git-repository-to-the-cluster",
    "title": "Module 0: Git and GitHub",
    "section": "Step 2: Connect git repository to the cluster",
    "text": "Step 2: Connect git repository to the cluster\nYou only need do this once. It downloads the repository from GitHub to your local machine or, if the directory already exists in your local machine, it links it to the github remote repository.\n\nOption 1: Clone (Do not do this one)\nUse this option if you are starting fresh and you don’t have an existing local repository. You already have a folder in the USC server so we don’t need to do this.\n# Navigate to where you want to keep your coursework\ncd /scratch1/username\n\n# Clone your specific repository (replace URL with your classroom repo link)\ngit clone https://github.com/trgn-515/USERNAME.git\n\n# Enter the directory\ncd USERNAME\n\n\nOption 2: Initiate existing folder\nSince you already have a folder for your user on the cluster, we don’t need to “clone” one from github. However, you need to turn your normal folder into a git repository.\n# 1. Enter your folder\ncd /scratch1/username\n\n# 2. Initialize Git\ngit init\ngit branch -M main\n\n# 3. Link to GitHub (Replace URL with your repo link)\ngit remote add origin https://github.com/trgn-515/USERNAME.git\n\n# 4. Pull any existing files (like the README) from GitHub\ngit pull origin main --allow-unrelated-histories",
    "crumbs": [
      "Home",
      "Module 0: Sequencing technologies",
      "Module 0: Git and GitHub"
    ]
  },
  {
    "objectID": "modules/0_intro_git/0_intro_git.html#step-1-pull-sync",
    "href": "modules/0_intro_git/0_intro_git.html#step-1-pull-sync",
    "title": "Module 0: Git and GitHub",
    "section": "Step 1: Pull (Sync)",
    "text": "Step 1: Pull (Sync)\nTo pull in git means to syncronize with the code from a remote repository (usually the latest version of the code).\nAlways start your work session with this. It ensures your local computer has the latest changes (e.g: you fixed a typo on the GitHub website or a collaborator merged a change).\ngit pull origin main",
    "crumbs": [
      "Home",
      "Module 0: Sequencing technologies",
      "Module 0: Git and GitHub"
    ]
  },
  {
    "objectID": "modules/0_intro_git/0_intro_git.html#step-2-edit-code",
    "href": "modules/0_intro_git/0_intro_git.html#step-2-edit-code",
    "title": "Module 0: Git and GitHub",
    "section": "Step 2: Edit code",
    "text": "Step 2: Edit code\nCreate files, write code, and save your work as you normally would. For this class, you will create a new folder for each assignment within your user repository.",
    "crumbs": [
      "Home",
      "Module 0: Sequencing technologies",
      "Module 0: Git and GitHub"
    ]
  },
  {
    "objectID": "modules/0_intro_git/0_intro_git.html#step-3-stage-and-commit-save",
    "href": "modules/0_intro_git/0_intro_git.html#step-3-stage-and-commit-save",
    "title": "Module 0: Git and GitHub",
    "section": "Step 3: Stage and commit (Save)",
    "text": "Step 3: Stage and commit (Save)\nGit has a two-step saving process.\n\nAdd (Stage): Select which files you want to save.\nCommit: Actually save them with a message describing what you did.\n\n# Check which files have changed\ngit status\n\n# Add everything in the current folder\ngit add .\n\n# or add a specific file\ngit add my_script.py\n\n# Save the snapshot with a message\ngit commit -m \"Completed exercise 1\"",
    "crumbs": [
      "Home",
      "Module 0: Sequencing technologies",
      "Module 0: Git and GitHub"
    ]
  },
  {
    "objectID": "modules/0_intro_git/0_intro_git.html#step-4-push-upload",
    "href": "modules/0_intro_git/0_intro_git.html#step-4-push-upload",
    "title": "Module 0: Git and GitHub",
    "section": "Step 4: Push (upload)",
    "text": "Step 4: Push (upload)\nYour commits currently live only on your computer. To submit your assignment, you must upload them to GitHub.\ngit push origin main",
    "crumbs": [
      "Home",
      "Module 0: Sequencing technologies",
      "Module 0: Git and GitHub"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TRGN 515: Advanced Human Genomic Analysis Methods",
    "section": "",
    "text": "Below is the schedule of modules, slides, and assignments for the semester. Links will be updated as we progress through the course.\n\n\n\n\n\n\n\n\n\nModule\nSlides\nAssignment\nDue Date\n\n\n\n\nModule 1: Sequencing TechnologiesOverview of NGS, Nanopore, and PacBio.\n HTML PDF\n Assignment 1\n02/27/2026\n\n\nModule 2: Structural Variant DetectionDefinition and types of structural variants (SV), detection of indels and SV.\n HTML PDF\n Assignment 2\n03/06/2026\n\n\nModule 3: Variant Functional AnnotationVariant effect prediction.\n HTML PDF\n Assignment 3\nTBD\n\n\nModule 4: Genome AssemblyIntroduction to genome assembly, QC, and hybrid-assembly.\n HTML PDF\n Assignment 4\nTBD\n\n\nModule 5: GWASIntroduction to GWAS, population structure, and linkage disequilibrium.\n HTML PDF\n Assignment 5\nTBD\n\n\nModule 6: Epigenetics - MethylationMethylation detection with 3rd Generation sequencing.\n HTML PDF\n Assignment 6\nTBD\n\n\nModule 7: Metagenomics IIntroduction to metagenomics, 16S amplification, and taxonomy.\n HTML PDF\n Assignment 7\nTBD\n\n\nModule 8: Metagenomics IIWhole-genome amplification, MAGs, and functional analysis.\n HTML PDF\n Assignment 8\nTBD"
  },
  {
    "objectID": "index.html#course-content",
    "href": "index.html#course-content",
    "title": "TRGN 515: Advanced Human Genomic Analysis Methods",
    "section": "",
    "text": "Below is the schedule of modules, slides, and assignments for the semester. Links will be updated as we progress through the course.\n\n\n\n\n\n\n\n\n\nModule\nSlides\nAssignment\nDue Date\n\n\n\n\nModule 1: Sequencing TechnologiesOverview of NGS, Nanopore, and PacBio.\n HTML PDF\n Assignment 1\n02/27/2026\n\n\nModule 2: Structural Variant DetectionDefinition and types of structural variants (SV), detection of indels and SV.\n HTML PDF\n Assignment 2\n03/06/2026\n\n\nModule 3: Variant Functional AnnotationVariant effect prediction.\n HTML PDF\n Assignment 3\nTBD\n\n\nModule 4: Genome AssemblyIntroduction to genome assembly, QC, and hybrid-assembly.\n HTML PDF\n Assignment 4\nTBD\n\n\nModule 5: GWASIntroduction to GWAS, population structure, and linkage disequilibrium.\n HTML PDF\n Assignment 5\nTBD\n\n\nModule 6: Epigenetics - MethylationMethylation detection with 3rd Generation sequencing.\n HTML PDF\n Assignment 6\nTBD\n\n\nModule 7: Metagenomics IIntroduction to metagenomics, 16S amplification, and taxonomy.\n HTML PDF\n Assignment 7\nTBD\n\n\nModule 8: Metagenomics IIWhole-genome amplification, MAGs, and functional analysis.\n HTML PDF\n Assignment 8\nTBD"
  },
  {
    "objectID": "modules.html",
    "href": "modules.html",
    "title": "Modules",
    "section": "",
    "text": "Here is a list of the course modules."
  },
  {
    "objectID": "modules.html#module-1",
    "href": "modules.html#module-1",
    "title": "Modules",
    "section": "Module 1",
    "text": "Module 1\n\nSequencing Technologies"
  },
  {
    "objectID": "modules.html#module-2",
    "href": "modules.html#module-2",
    "title": "Modules",
    "section": "Module 2",
    "text": "Module 2\n\nStructural variants"
  },
  {
    "objectID": "modules.html#module-3",
    "href": "modules.html#module-3",
    "title": "Modules",
    "section": "Module 3",
    "text": "Module 3\n\nVariant consequence annotation"
  },
  {
    "objectID": "modules/0_intro_git/assignment_template.html",
    "href": "modules/0_intro_git/assignment_template.html",
    "title": "Assignment X: Topic Name",
    "section": "",
    "text": "Markdown is a lightweight markup language that allows you to format text using plain text symbols. It is the industry standard for documentation in bioinformatics and data science. It is widely used primarily due to the following characteristics:\n\nCode-Friendly: It handles code blocks and syntax highlighting natively.\nGit-Friendly: Since it is plain text, Git can track changes line-by-line (unlike Word documents).\nUniversal: It renders automatically on GitHub and converts easily to PDF or HTML."
  },
  {
    "objectID": "modules/0_intro_git/assignment_template.html#date-last-modified",
    "href": "modules/0_intro_git/assignment_template.html#date-last-modified",
    "title": "Assignment X: Topic Name",
    "section": "",
    "text": "Markdown is a lightweight markup language that allows you to format text using plain text symbols. It is the industry standard for documentation in bioinformatics and data science. It is widely used primarily due to the following characteristics:\n\nCode-Friendly: It handles code blocks and syntax highlighting natively.\nGit-Friendly: Since it is plain text, Git can track changes line-by-line (unlike Word documents).\nUniversal: It renders automatically on GitHub and converts easily to PDF or HTML."
  },
  {
    "objectID": "modules/0_intro_git/assignment_template.html#this-is-a-subheader",
    "href": "modules/0_intro_git/assignment_template.html#this-is-a-subheader",
    "title": "Assignment X: Topic Name",
    "section": "This is a subheader",
    "text": "This is a subheader\nStart each answer pasting the question as given in the pdf. After that, you can add your answer."
  },
  {
    "objectID": "modules/0_intro_git/assignment_template.html#writing-code",
    "href": "modules/0_intro_git/assignment_template.html#writing-code",
    "title": "Assignment X: Topic Name",
    "section": "Writing code",
    "text": "Writing code\nIf the answer requires code, use a code block starting and ending with single quotation marks like the one below:\n# Write your code here\nYou can add language specific code blocks by using the language’s name:\n# Write your BASH code here\nsamtools view ...\n# Write your R code here\nm1 = glm(...)\n# Write your Python code here\nimport pandas"
  },
  {
    "objectID": "modules/0_intro_git/assignment_template.html#adding-images",
    "href": "modules/0_intro_git/assignment_template.html#adding-images",
    "title": "Assignment X: Topic Name",
    "section": "Adding images",
    "text": "Adding images\nTo add an image or plot to your answer, save the image in your folder and use the following code:\n\nTo add a caption to your image:\n\n\n\nCaption text\n\n\nIf the image is not in the same folder as your markdown file, you need to put the whole path:\n\nThe square brackets are used to give the image a specific format. For instance:"
  },
  {
    "objectID": "modules/0_intro_git/assignment_template.html#other-important-syntax",
    "href": "modules/0_intro_git/assignment_template.html#other-important-syntax",
    "title": "Assignment X: Topic Name",
    "section": "Other important syntax",
    "text": "Other important syntax\nYour answer can also include lists. You can start a list with the “*” symbol for a bulleted list or with numbers for a numbered list.\n\nItem 1\nItem 2\n\nOr\n\nItem 1\nItem 2\n\nYou can also make a word italic or bold.\nOnce you finish an answer, put the next question on a different page:"
  },
  {
    "objectID": "modules/0_intro_git/assignment_template.html#adding-tables",
    "href": "modules/0_intro_git/assignment_template.html#adding-tables",
    "title": "Assignment X: Topic Name",
    "section": "Adding tables",
    "text": "Adding tables\nIf you need to include a table, you can do it like this:\n\n\n\nColumn 1\nColumn 2\n\n\n\n\nText 1\nText 2\n\n\nText 3\nText 4\n\n\n\nMaking tables can be tedius. My favority online software to turn .csv or excel tables into markdown or latex is https://www.tablesgenerator.com/markdown_tables"
  },
  {
    "objectID": "modules/1_sequencing_techs/1_sequencing_techs.html",
    "href": "modules/1_sequencing_techs/1_sequencing_techs.html",
    "title": "Assignment 1: Next Generation Sequencing Technologies",
    "section": "",
    "text": "DNA sequencing can be broadly defined as the determination of the identity and order of nucleic acid residues in biological samples. Bioinformatic analysis and sequencing results are greatly affected by the choice of sequencing technlogy. Sequencing technologies can be broadly categorized in three groups:\nWe will use a human genome standard (HG002) that has been extremely well-characterized as our gold standard. The HG002 genome assembly is part of an effort hosted by NIST that includes The Telomere-to-Telomere Consortium, the Human Pangenome Reference Consortium and the Genome in a Bottle Consortium, to sequence, assemble and polish the HG002 (also known as GM24385 and huAA53E0) cell line, creating a human “genome benchmark” for the HG002 reference material that covers all bases of the diploid genome and is perfectly accurate. Hence, their own “Q100” project nickname, which refers to a Phred quality score of 1 error per 10 billion bases.\nSome reference papers regarding this dataset:\nWe will compare the characteristics and SNP calls for three sequencing technologies: Illumina, PacBio HiFi, and Nanopore.",
    "crumbs": [
      "Home",
      "Module 1: Sequencing technologies",
      "Assignment 1: Next Generation Sequencing Technologies"
    ]
  },
  {
    "objectID": "modules/1_sequencing_techs/1_sequencing_techs.html#quality-check-for-short-reads---fastqc",
    "href": "modules/1_sequencing_techs/1_sequencing_techs.html#quality-check-for-short-reads---fastqc",
    "title": "Assignment 1: Next Generation Sequencing Technologies",
    "section": "Quality check for short-reads - FastQC",
    "text": "Quality check for short-reads - FastQC\nThe most widely used tool for visually evaluating fastq data is FastQC. If FastQC is ran without arguments, it will open an interactive GUI version of the software. But in most cases, we will run it in the command line.\nA basic way to call FastQC will the following command:\nfastqc # The main command call\n--extract # Extract files from output\n-o &lt;output_dir&gt; # Directory for output files\n-d &lt;temporary_dir&gt; # Directory for temporary files\n&lt;input.fq&gt; # Input fasta file\nThis code can be embedded within a python script or a cluster slurm batch job. An easy way to run it within bash in a batch script would be:\n#!/bin/bash\n\n#SBATCH --account=msalomon_1385\n#SBATCH --partition=main\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=5G\n#SBATCH --time=1:00:00\n#SBATCH --job-name=fastqc\n#SBATCH --output=/scratch1/USER/temp/fastqc_%j.out\n#SBATCH --error=/scratch1/USER/temp/fastqc_%j.err\n\n# Put the input and output directories into variables for conciseness\nin_dir=/project/msalomon_1385/TRGN_515/1_exp_evolution/data/m_tuberculosis/fastq\nout_dir=/scratch1/USER/1_exp_evolution/fastq_qc\n\n# Then we loop over all files within the directory\n\nfor in_fa in $in_dir/*.fastq.gz; # Initiate loop\n    sample=$(basename $in_fa .fastq.gz) # Set up a sample variable for the output.\n    fastqc -o $out_dir/$sample --extract --dir $out_dir $in_fa\ndone # Finilize loop\nBut remember, this is just one way to run it!",
    "crumbs": [
      "Home",
      "Module 1: Sequencing technologies",
      "Assignment 1: Next Generation Sequencing Technologies"
    ]
  },
  {
    "objectID": "modules/1_sequencing_techs/1_sequencing_techs.html#quality-check-for-long-reads",
    "href": "modules/1_sequencing_techs/1_sequencing_techs.html#quality-check-for-long-reads",
    "title": "Assignment 1: Next Generation Sequencing Technologies",
    "section": "Quality check for long-reads",
    "text": "Quality check for long-reads\nYou can visualize the quality of your long reads using FastQC as you would with your short-reads. However, there’s also specific software built for long-reads. A widely used software is NanoPlot. Another option is LongQC.\n\n\n\n\n\n\nWarning\n\n\n\nIf you use FastQC on very long reads, it may run out of memory. By default, FastQC allocates a very small amount of memory (512MB), which makes it run out of memory with very long reads. The way to fix this issue is to allocate more memory using the --memory flag. So if you use FastQC, allocate 5Gb, which should be more than enough. You can do this with the command:\nfastqc --extract -o &lt;output_dir&gt; --d &lt;temporary_dir&gt; --memory 5000 &lt;input.fq&gt;\n\n\nNanoPlot is equally easy to run. It will give similar output to FastQC, but more centered on read length and base quality, which is often the obsession with Nanopore long reads.\nNanoPlot\n--fastq &lt;input.fq&gt; # Input in fastq file\n-o &lt;output_dir&gt; # Output directory\n--loglength # Set up for log read length in the plots\n\nTask 2: Run FastQC on Illumina reads\n\n\n\n\n\n\nNote\n\n\n\nRun FastqQC on the Illumina reads and answer the questions. Focus only on the following figures:\n\nPer base sequence quality\nPer sequence quality scores\nPer base sequence content\nPer sequence GC content\nPer base N content\nSequence Length Distribution\nOverrepresented sequences\nAdapter Content\n\nAnswer the following questions for each plot:\n\nWhat is the plot representing? (10pts)\nWhat quality issue (if any) is the plot showing? (10pts)\nHow would you solve the issue (if any) represented in the plot? (10pts)\n\nAnswer the questions for the forward reads only.",
    "crumbs": [
      "Home",
      "Module 1: Sequencing technologies",
      "Assignment 1: Next Generation Sequencing Technologies"
    ]
  },
  {
    "objectID": "modules/1_sequencing_techs/1_sequencing_techs.html#qc-for-short-reads---trimmomatic",
    "href": "modules/1_sequencing_techs/1_sequencing_techs.html#qc-for-short-reads---trimmomatic",
    "title": "Assignment 1: Next Generation Sequencing Technologies",
    "section": "QC for short-reads - Trimmomatic",
    "text": "QC for short-reads - Trimmomatic\nMultiple tools are commonly used to perform QC on short-reads: 1. Trimmomatic - https://github.com/usadellab/Trimmomatic 2. fastp - https://github.com/OpenGene/fastp 3. Trim Galore! - https://github.com/FelixKrueger/TrimGalore\nWe will use Trimmomatic for this. To remove adapters, you need to provide a file with the adapters for the Illumina machine the reads come from. If you installed Trimmomatic through a conda distribution, that should be within: anaconda3/share/trimmomatic/adapters/. You can also find it at their github page.\nTo run trimmomatic:\ntrimmomatic PE # Main trimmomatic function for paired-ends reads\n&lt;input_1.fastq.gz&gt; # Forward reads\n&lt;input_2.fastq.gz&gt; # Reverse reads\n&lt;output_1.fastq.gz&gt; # Output forward reads\n&lt;output_unclassified_1.fastq.gz&gt; # Output unclassified reads from forward file\n&lt;output_2.fastq.gz&gt; # Output reverse reads\n&lt;output_unclassified_2.fastq.gz&gt; # Output unclassified reads from reverse file\nSLIDINGWINDOW:&lt;window_size&gt;:&lt;quality&gt; # Sliding window for quality trimming.\nLEADING:&lt;quality&gt; # Remove leading bases under quality\nTRAILING:&lt;quality&gt; # Remove trailing bases under quality\nAVGQUAL:&lt;quality&gt; # Average quality to remove a read\nMINLEN:&lt;length&gt; # Minimum length to remove a read\nILLUMINACLIP:&lt;/path/to/adapter_file.fa&gt;:&lt;seed_mismatches&gt;:palindrome_clip_threshold&gt;:&lt;simple_clip_threshold&gt;\nFor the SLIDINGWINDOW argument, a window of window_size size moves along the read. If the average quality of the window falls below quality, the remaining of the read is clipped. This prevents reads to be removed just because of a few bad bases. This argument assumes the quality will only get worse at the end of the read.\nThe LEADING and TRAILING arguments will start at the begonning and end of the read and keep removing bases as long as their quality is below quality.\nFor ILLUMINACLIP, we need to provide the file for the adapters. This depends on the Illumina machine. The seed_mismatches argument specifies the maximum number of base mismatches allowed within a short initial sequence from the read (“seed”, 16bp) used to identify potential adapter matches, allowing for a small degree of mismatch when searching for adapters to clip. Trimmomatic uses two strategies to find adapters: the sample mode, and the palindromic mode. simple_clip_threshold is the minimum score threshold for the adapter to align to the read for clipping to take place in the simple mode. Suggested values are 7-15. palindrome_clip_threshold specifies how accurate the match between the two ‘adapter ligated’ reads must be for paired-end palindrome read alignment. Suggested values are around 30. For more information, these two strategies are extensively explained in the Trimmomatic publication.\nFor our dataset, we can run Trimmomatic within bash as follows:\nin_dir=/project/msalomon_1385/TRGN_515/1_seq_techs/illumina\nout_dir=/scratch1/USER/1_exp_evolution/fastq_pass\nadapters=/path/to/conda/share/trimmomatic/adapters/TruSeq3-PE.fa\n\n# Unlike FastQC, we need to run both forward and reverse reads together\n\nfor file in $in_dir/*_1.fastq.gz; # Initiate loop with forward reads only\n    sample=$(basename $in_fa _1.fastq.gz) # Set up a sample variable for input and output.\n    fwd=$in_dir/${sample}_1.fastq.gz # Forward read\n    rev=$in_dir/${sample}_2.fastq.gz # Reverse read\n    trimmomatic PE $fwd $rev $out_dir/${sample}_1.fastq.gz /dev/null $out_dir/${sample}_2.fastq.gz /dev/null SLIDINGWINDOW:4:5 LEADING:5 TRAILING:5 AVGQUAL:5 MINLEN:35 ILLUMINACLIP:$adapters:2:30:10\n    # Note we send unclassified reads to /dev/null\ndone # Finilize loop\n\nTask 3: Run FastQC after running Trimmomatic\n\n\n\n\n\n\nNote\n\n\n\nCompare the following FastqQC plots before and after:\n\nPer base sequence quality\nPer sequence quality scores\nPer base sequence content\nPer sequence GC content\nPer base N content\nSequence Length Distribution\nOverrepresented sequences\nAdapter Content\n\nAnswer the following questions for each plot:\n\nWhat has changed after running trimmomatic? (10pts)\nWhy did the change happen? (10pts)",
    "crumbs": [
      "Home",
      "Module 1: Sequencing technologies",
      "Assignment 1: Next Generation Sequencing Technologies"
    ]
  },
  {
    "objectID": "modules/1_sequencing_techs/1_sequencing_techs.html#short-read-mapping",
    "href": "modules/1_sequencing_techs/1_sequencing_techs.html#short-read-mapping",
    "title": "Assignment 1: Next Generation Sequencing Technologies",
    "section": "Short-read mapping",
    "text": "Short-read mapping\n\nBWA\nTo run any read mapping software, we first need a reference genome. We will use the Human Reference Genome version GRCh38. You can find the CARC location for this file at the beginning of this document.\nMost modern mappers require building an index to easily parse the reference genome during mapping. In fact, some of the biggest speed and memory improvements over early mappers was thanks to improved indexing methods. If they don’t require an index file, it’s because they index the reference genome on the fly. BWA, for instance, is based on the Burrows-wheeler Transform for powerful indexing to quickly locate where a read might align within the reference genome. The indexing for BWA can be done with the following command:\nbwa index $ref_genome\nThat will create multiple files with the same name as your reference genome, but ending in different extensions. Those are the indexes. Keep those files with the same and location as your reference genome. If a software requires those indexes, you just need to show the location of the reference genome, and it will assume the name and location of all the indexes.\nOn CARC there’s already a folder with the indexed GRCh38 reference genome, so we don’t need to do this step.\nThe basic use of BWA requires only the indexed reference genome and the files for the reads (or single file if not paired-end). To run BWA, do:\nbwa mem # Main call for the mem algorithm within BWA\n&lt;reference_genome&gt; # Reference genome\n&lt;fastq1&gt; # Forward reads\n&lt;fastq2&gt; # Reverse reads\nAn important part of the BAM file is the Read Group, especially for Illumina reads. Read groups provide technical information about flowcell, lane and multiplexing of illumina reads. This is important for finding library and sequencing issues, as well as for removing duplicates (next section!). Read Groups are added within the header with the tag RG. To find the flowcell and lane information you can use the headers of a fastq file. For modern fastq files:\nflowcell=$(zcat &lt;file.fq.gz&gt; | head -n1 | cut -d':' -f3)\nlane=$(zcat &lt;file.fq.gz&gt; | head -n1 | cut -d':' -f4)\nTwo common flags to add are the Read Group ID abd the Platform unit. You can define them as:\nrg_id=$flowcell.$lane\npu=$flowcell.$lane.$sample\nWhere $sample is your sample name. To add read groups on bwa mem, use the -R flag.\nbwa mem -R \"@RG\\tID:$rg_id\\tPU:$pu\\tPL:$pl\\tSM:$sample\" \nThe read group ID and platform information allows us to find library and sequencing bias. The sample name makes the BAM and VCF file a lot more concise later on, as the sample name is extracted from there.\nFastq files generated with older machines have different headers. You can find more information about the Fastq headers here (https://en.wikipedia.org/wiki/FASTQ_format) and about Read groups in the GATK website (https://gatk.broadinstitute.org/hc/en-us/articles/360035890671-Read-groups).\n\n\n\n\n\n\nWarning\n\n\n\nSome fastq files may have more than one flowcell and lane for all the reads if the sample was sequenced in multiple flowcell lanes. In that case, if you detect bias in the Per Tile plot in FastQC, you may have to separate the reads by flowcell and lane before mapping.\n\n\nThe output of bwa is sometimes unsorted. To ensure we have a coordinate sorted bam file (sorted by starting position), do the following:\nsamtools sort # Main call\n-@ &lt;n&gt; # Number of threads\n-T &lt;path&gt; # Temporary directory\n-O bam # Output in bam format\n-o &lt;path&gt; # Output directory\nintput_bam # Input bam file\nsamtools sort allows for other types of sorting, such as read name sorting or tag-based sorting, but coordinate sorting is by far the most common and expected by downstream bioinformatic processes.\nMany tools will require to index the output bam file for efficiency. This can be done with:\nsamtools index bam_file;\nWe can combine everything into one command using bash pipes:\nbwa mem &lt;reference_genome&gt; &lt;fastq1_path&gt; &lt;fastq2_path&gt; | samtools view -bS - | samtools sort -@ &lt;n&gt; -T &lt;path&gt; -O bam -o &lt;out_bam_path&gt;\n\nsamtools index &lt;out_bam_path&gt;",
    "crumbs": [
      "Home",
      "Module 1: Sequencing technologies",
      "Assignment 1: Next Generation Sequencing Technologies"
    ]
  },
  {
    "objectID": "modules/1_sequencing_techs/1_sequencing_techs.html#marking-duplicates",
    "href": "modules/1_sequencing_techs/1_sequencing_techs.html#marking-duplicates",
    "title": "Assignment 1: Next Generation Sequencing Technologies",
    "section": "Marking Duplicates",
    "text": "Marking Duplicates\nThe term “duplicates” usually refers to identical sequences that appear more than once in the data. Duplicated sequences can be genuine, meaning that they are created due to real duplicated fragments in the genome. However, these duplicates can also occur due to library prep and sequencing artifacts. High rate of artificial duplicates can impact variant detection, as a variant may seem supported by many independent reads when in reality it’s just an error artificially duplicated.\nThe two most common sources for artificial duplicates in Illumina data are: * PCR duplicates - They occur when the same fragment of DNA gets amplify multiple times during library preparation, often due to overamplification * Optical duplicates - They occur during sequencing when a single amplification fluorescent cluster is incorrectly detected as multiple clusters by the optical sensor of the sequencing instrument.\n\n\n\n\n\n\nWarning\n\n\n\nDuplicate marking is not always required! Some amplicon sequencing approaches will have sequencing reads start at the same sites (the primer sites), and therefore could be marked as duplicates without representing errors. Also, in RNA-seq you would also expect high rates of sample duplication, and therefore is also not advised to remove duplicates (unless you have UMIs…)\n\n\nMost tools for marking duplicates from BAM files use the 5’ coordinates and mapping orientations of each read (or read pair), including any clipping, gaps or insertions, finding all reads that share exact genomic coordinates. It then keeps the read with the highest quality, marking the rest as duplicates. Note that often reads are not removed, but a flag is added. That’s why we call it marking duplicates!\nThe most commonly used software for marking duplicates is Picard (also implemented within GATK). However, this tool requires a lot of memory and time. I prefer another implementation of the same algorithm within the software Sambamba.\n\nSambamba\nSambamba takes a bam file as input and outputs a bam file where the read flag has been changed in some reads to reflect they are a duplicate. You can run sambamba as:\nsambamba markdup # Main call to the duplicate marking function\n--tmpdir=&lt;temp_dir&gt;\n&lt;input.bam&gt;\n&lt;output.bam&gt;\nUnfortunately, sambamba (as picard and most tools for duplicate detection) require multiple passes of the input bam file, and therefore can’t read from stdin and be used in a pipe.\nFor a efficient and fast software that can be included within pipes, you can use samtools markdup. To run samtools markup you need to do a couple of operations to ensure proper read sorting and tagging:\nsamtools collate -Ou &lt;input_bam&gt; | # Order reads by read-pairs\nsamtools fixmate -m - - -u | # Add mate coordinates and score tags\nsamtools sort - -u | # Order reads by coordinate\nsamtools markdup - &lt;output_bam&gt; # Mark duplicates\nThis code can be pipped into your previous mapping command into one nice and efficient stream!",
    "crumbs": [
      "Home",
      "Module 1: Sequencing technologies",
      "Assignment 1: Next Generation Sequencing Technologies"
    ]
  },
  {
    "objectID": "modules/1_sequencing_techs/1_sequencing_techs.html#long-read-mapping",
    "href": "modules/1_sequencing_techs/1_sequencing_techs.html#long-read-mapping",
    "title": "Assignment 1: Next Generation Sequencing Technologies",
    "section": "Long-read mapping",
    "text": "Long-read mapping\nThe most widely long-read mapper is minimap2, written by the same person who developed BWA, Heng Li. As BWA, the command line command to run it is quite simple:\nminimap2\n&lt;reference_genome&gt;\n&lt;long_reads.fastq.gz&gt;\nminimap2 does not require you to provide an index, as it does it very efficienty on the fly.\nA useful feature of minimap2 is that it comes with some presets depending on the sequencing platform and the type of read. These presets take into account the typical read length and sequencing error profile for each technology.\n\nTask 5: Map reads to the reference genome\n\n\n\n\n\n\nNote\n\n\n\nUse bwa mem for short reads and minimap2 for long-reads. For minimap2, choose the appropiate platform in the presets.\nIn both cases, create a pipe with the output of the mapper and samtools for coordinate sorting.\nFor the final output, any read that maps to chromosomes other than the chromosome you are working with should be considered ambiguous. Use the right command to keep only reads that mapped to your chromosome of interest.\n\nCreate a slurm job to run the previous code. Attach your code to the assignment (10pts)",
    "crumbs": [
      "Home",
      "Module 1: Sequencing technologies",
      "Assignment 1: Next Generation Sequencing Technologies"
    ]
  },
  {
    "objectID": "modules/1_sequencing_techs/1_sequencing_techs.html#variant-calling-in-short-reads---bcftools",
    "href": "modules/1_sequencing_techs/1_sequencing_techs.html#variant-calling-in-short-reads---bcftools",
    "title": "Assignment 1: Next Generation Sequencing Technologies",
    "section": "Variant calling in short-reads - BCFtools",
    "text": "Variant calling in short-reads - BCFtools\nFor Illumina reads, we will use BCFtools for SNP calling.\nBcftools is ran in two steps. First, we create a pileup of the alignment. A pileup is a file where aligned reads are parsed and summarized at each specific genomic position. Thus, each genomic position is scanned and reads that spanned that position are gathered (piled up!), summarizing that position with metrics such as read depth, base quality, position bias, etc. It’s basically like finding variants but without applying any specific model. Many variant calling tools use some sort of pileup, even if they don’t do it explicitly as bcftools. Typically, the output of the pileup will be pippeted to bcftools variant calling function, which applies a model to calculate the likelihood of a variant and a genotype, given the pileup at that position. Variants are often stored in a Variant Calling Format (VCF) file.\nTo run the pileup and bcftools for variant discovery:\nbcftools mpileup # Pileup function\n-f &lt;reference_genome&gt;\n-a AD,INFO/AD,ADF,INFO/ADF,ADR,INFO/ADR,DP,SP # Add tags useful for filtering\n-q &lt;n&gt; # Skip reads with read quality below n\n-Q &lt;n&gt; # Skip bases with base quality below n\n-Ou # Uncompressed output\n&lt;input_bam&gt;\n&lt;output_vcf&gt;\nOnce the pileup is done, the second step involves running the actual variant calling model. The main variant calling function can be used as:\nbcftools call # Main bcftools call function\n-m # Multiallelic calling method\n-Oz # Output in compressed format\n&lt;output.vcf.gz&gt;\nAfter calling variants, the file needs to be normalized. Variant normalization ensures genetic variants are represented in a consistent way across different data sets.\nbcftools norm\n-f &lt;reference_genome&gt;\n-Oz # Output in compressed format\n-o &lt;output.vcf.gz&gt; # Output file name\n&lt;input.vcf.gz&gt; # Input file name\nAfter calling and normalizing variants, we can soft-filter them to tag variants that we deem of low quality. Soft-filtering means that we won’t remove low-quality variants, we will just add a tag to them.\nbcftools filter # Main function\n-m+ # Append filter tags to previous tags\n# Then each tag is added depending on the condition tested, and we piped it to more filtering commands\n-s'MinMQ' # Tag to add\n-e 'INFO/MQ &lt; 20' # Condition to add tag\n&lt;input.vcf.gz&gt; | # Input file name\nbcftools filter -m+ -s'QUAL' -e 'QUAL &lt; 20' |\nbcftools filter -m+ -s'minAD' -e 'FMT/AD[:1] &lt; 10' |\n# ... as many filters as you want |\nbcftools filter -m+ -s'minADF' -e 'FMT/ADF[:1] &lt; 3' |\n-Oz # Output in compressed format\n-o &lt;output.vcf.gz&gt; # Output file name\nMany tools will require the VCF file to be indexed. This can be done as follows:\ntabix -p vcf &lt;vcf_file&gt;\n\nTask 6: Complete the Variant Calling command:\n\n\n\n\n\n\nNote\n\n\n\nPipe the commands mpileup, call, norm and filter into one command to stream from the input bam file to the final VCF output.\nbcftools mpileup | ...\nUse the commands following this instructions. You will have to look at the help for each command:\n\nFor bcftools mpileup:\n\nUse all reads regardless of mapping quality\nKeep only bases with base quality higher or equal than 20\n\nFor bcftools call:\n\nPrint variant sites only\nDo not report indels\nKeep all alternate alleles\n\n\nApply the following filter tags in bcftools filter:\n\n‘MinMQ’ for MQ lower than 20\n‘QUAL’ for QUAL lower than 20\n‘minAD’ for allele depth lower than 20\n‘minADF’ for allele forward depth lower than 5\n‘minADR’ for allele reverse depth lower than 5\n‘MinDP’ for total depth lower than 50\n\n\n\nYour output should be a VCF with the same number of variants as before. Remember, we have not removed any variant, we have only added a filter tag in the FILTER column. Variants that pass all our filters will automatically be assigned the tag “PASS” in the FILTER column.",
    "crumbs": [
      "Home",
      "Module 1: Sequencing technologies",
      "Assignment 1: Next Generation Sequencing Technologies"
    ]
  },
  {
    "objectID": "modules/1_sequencing_techs/1_sequencing_techs.html#long-read-variant-calling",
    "href": "modules/1_sequencing_techs/1_sequencing_techs.html#long-read-variant-calling",
    "title": "Assignment 1: Next Generation Sequencing Technologies",
    "section": "Long-read variant calling",
    "text": "Long-read variant calling\nTechnically, there’s no need to use different software for long-read variant calling. However, many variant callers have been developed with long-reads in mind, and benchmarking studies show they can perform better than general variant callers. Also, long-read mapping is characterized by alignments full of small artefactual indels, which can make short-read variant callers struggle and take a long time.\n\nTask 7: Call SNPs against the human reference genome**\n\n\n\n\n\n\nNote\n\n\n\nTo be consistent between sequencing platforms, we will use a variant caller that’s platform agnostic (i.e: it’s not optimized for any especific sequencing technology or read type). We will use BCFtools.\nYou can choose any variant caller you prefer, but run the same caller for all three sequencing technologies (Illumina, PacBio HiFi, ONT). Use any filters you see necessary.\nFor this task, report your variant calling command and your choice of options and filters.\n\nCreate a slurm job to run the SNP calling code. Call only SNPs. Attach your code to the assignment (5pts)",
    "crumbs": [
      "Home",
      "Module 1: Sequencing technologies",
      "Assignment 1: Next Generation Sequencing Technologies"
    ]
  }
]